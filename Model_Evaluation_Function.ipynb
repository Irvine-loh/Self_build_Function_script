{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "Copy of Model_Evaluation_Function.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16afd4e8"
      },
      "source": [
        "## Model Evaluation Function\n",
        "### Description : Contains Different Type of model performance evalution function\n",
        "\n",
        "#### Script Version : 1.0.0\n",
        "#### Script Written by : Loh Khai Shyang \n",
        "#### Script Date : 22 Nov 2021\n",
        "#### Contact Email : irvinekhai@gmail.com"
      ],
      "id": "16afd4e8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecf29306"
      },
      "source": [
        "def XGB_Regression_model_evaluation_function (model,label,over_under_reject_analyze_feature,output_spec_limits,train_df,test_df,X_train,x_test,Y_train,y_test):\n",
        "    ### 1. Plot Model performance RMSE of training and validation set\n",
        "    ### 2. Confusion matrix scatter plot of \"training data\" and \"testing data\" [ evaluate how good is the model performance on training and testing ]\n",
        "        # use train model re-predict \"training data \" and plot scatter confusion matrix\n",
        "    ### 3. Plot Over Reject Graph \n",
        "    ### 4. Plot Over Reject Graph \n",
        "    \n",
        "    ### Hyper Parameter\n",
        "        # 1. model = Trained model \n",
        "            # NOTE !! Model XGb must have \"eval_set\"\n",
        "            # \"eval_set\" can have eval_set=[(X_train,Y_train)] or eval_set=[(X_train,Y_train),(x_test,y_test)]\n",
        "            # Example : model.fit(X_train,Y_train, early_stopping_rounds=20, eval_set=[(X_train,Y_train),(x_test,y_test)], eval_metric='rmse')\n",
        "        # 2. label = Y output name\n",
        "        # 3. output_spec_limits = output spec limit\n",
        "                # is a list = [lower spec limit , upper spec limit ]\n",
        "                # if no spec = 'Null' , Ex : [ 35, 'Null']\n",
        "\n",
        "        ### NOTE !!! [ Dataset below consist of all original value from original dataset ]\n",
        "        # 4. train_df = include all features in dataset  [ exclude output feature columns] --- [ Note!!!   training_df + testing_df = original dataset ]\n",
        "        # 5. testing_df = include all features in dataset  [ exclude output feature columns] --- [ Note!!!   training_df + testing_df = original dataset ]\n",
        "        \n",
        "        ### NOTE !!! [ Dataset below is ready to fit into model for training - done normalization/stadardization or getdummies]\n",
        "        # 6. X_train / x_test = dataframe include input features that are necessary to train the model \n",
        "        # 7. Y_train / y_test = dataframe include output features that are necessary to train the model\n",
        "\n",
        "    \n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns \n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.lines as mlines\n",
        "\n",
        "    \n",
        "    def _mean_max_stdev_calculate_function (variable):\n",
        "        ### Calculate and return \"variable\" : Min, Max, Mean, Stdev ###\n",
        "        \n",
        "        mean = round(np.mean(variable),2)\n",
        "        stdev =round(np.std(variable),2)\n",
        "        maximum = round(np.max(variable),2)\n",
        "        minimum = round(np.min(variable),2)\n",
        "        \n",
        "        return minimum,maximum,mean,stdev\n",
        "    \n",
        "    \n",
        "    def _evaluate_train_test_model_function(model,label,output_spec_limits,train_df,test_df,X_train,x_test,Y_train,y_test):\n",
        "        ### Plot 1. Train Test Confusion Matrix Plot \n",
        "        ### Plot 2. Distplot with Distibution Plot [Absolute_Residual = abs(\"Predict\" - \"Actual\")] ( Turn on to draw ) \n",
        "        ### Plot 3. Distplot with Density Plot ( Turn on to draw ) \n",
        "        ### Plot 4. Distplot with Distibution Plot [Residual = \"Predict\" - \"Actual\"] ( Turn on to draw ) \n",
        "    \n",
        "        \n",
        "        ### Return train_df, test_df\n",
        "            # Train_df = training dataset consist of \"Actual Output column\" and \"Predicted Output column\"\n",
        "            # Test_df = Testing dataset consist of \"Actual Output column\" and \"Predicted Output column\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        ##  1. Plot 1. Train Test Confusion Matrix Plot  ##\n",
        "        \n",
        "        train_pred = model.predict(X_train)\n",
        "        test_pred = model.predict(x_test)\n",
        "        \n",
        "        # Statistical value #\n",
        "        train_actual_min, train_actual_max, train_actual_mean, train_actual_stdev = _mean_max_stdev_calculate_function(Y_train)\n",
        "        train_pred_min, train_pred_max, train_pred_mean, train_pred_stdev = _mean_max_stdev_calculate_function(train_pred)\n",
        "        \n",
        "        test_actual_min, test_actual_max, test_actual_mean, test_actual_stdev = _mean_max_stdev_calculate_function (y_test)\n",
        "        test_pred_min, test_pred_max, test_pred_mean, test_pred_stdev = _mean_max_stdev_calculate_function (test_pred)\n",
        "        \n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(15,15))\n",
        "        if output_spec_limits[0] != 'Null' :\n",
        "            ax.axvline(output_spec_limits[0], \n",
        "                       linestyle='--',\n",
        "                       color='r' , \n",
        "                       label='Spec L.limit:'+str(output_spec_limits[0]))  # Plot Actual Ground Truth Lower Limit line\n",
        "            ax.axhline(output_spec_limits[0], \n",
        "                       linestyle='--',\n",
        "                       color='r') # Predicted Ground Truth Value\n",
        "        if output_spec_limits[1] != 'Null' :\n",
        "            ax.axvline(output_spec_limits[1], \n",
        "                       linestyle='--',\n",
        "                       color='m',\n",
        "                       label='Spec L.limit:'+str(output_spec_limits[1]))  # Plot Actual Ground Truth Lower Limit line\n",
        "            ax.axhline(output_spec_limits[1], \n",
        "                       linestyle='--',\n",
        "                       color='m') # Predicted Ground Truth Value\n",
        "            \n",
        "        ax.scatter(Y_train, \n",
        "                   train_pred, \n",
        "                   color = 'b', \n",
        "                   marker = 'o',\n",
        "                   label = label+'_Train_Actual [ mean:'+str(train_actual_mean)+', std:'+str(train_actual_stdev)+', max:'+str(train_actual_max)+', min:'+str(train_actual_min)+']\\n'+label+'_Train_Pred [ mean:'+str(train_pred_mean)+', std:'+str(train_pred_stdev)+', max:'+str(train_pred_max)+', min:'+str(train_pred_min)+']'\n",
        "                  ) # Scatter plot Training Dataset of \"actual vs predicted\"\n",
        "        ax.scatter(y_test, \n",
        "                   test_pred, \n",
        "                   color='r', \n",
        "                   marker ='o',\n",
        "                   label = label+'_Test_Actual [ mean:'+str(test_actual_mean)+', std:'+str(test_actual_stdev)+', max:'+str(test_actual_max)+', min:'+str(test_actual_min)+']\\n'+label+'_Test_Pred [ mean:'+str(test_pred_mean)+', std:'+str(test_pred_stdev)+', max:'+str(test_pred_max)+', min:'+str(test_pred_min)+']'\n",
        "                  ) # Scatter plot Testing Dataset of \"actual vs predicted\"\n",
        "        plt.xlabel('Actual '+label)\n",
        "        plt.ylabel('Predicted '+label)\n",
        "        plt.title('['+label+'] Train Test Confusion Matrix Scatter Plot')\n",
        "        plt.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1, fontsize=8)\n",
        "        \n",
        "        \n",
        "        \n",
        "        ## 2. Plot Residual train test Distribution/Density plot  ##\n",
        "            # Residual = absolute ( \"Y_actual_value\" - \" Y_Predicted_value\")\n",
        "            \n",
        "        train_df[label+'_Actual'] = Y_train\n",
        "        train_df[label+'_Predict'] = train_pred\n",
        "        train_df[label+'_Residual'] = train_df[label+'_Predict'] - train_df[label+'_Actual']  \n",
        "        train_df[label+'_Absolute_Residual'] = abs(train_df[label+'_Predict'] - train_df[label+'_Actual'] )\n",
        "        \n",
        "        test_df[label+'_Actual'] = y_test\n",
        "        test_df[label+'_Predict'] = test_pred\n",
        "        test_df[label+'_Residual'] = test_df[label+'_Predict'] - test_df[label+'_Actual']\n",
        "        test_df[label+'_Absolute_Residual'] = abs(test_df[label+'_Predict'] - test_df[label+'_Actual'])\n",
        "        \n",
        "        \n",
        "        train_min, train_max, train_mean, train_stdev =_mean_max_stdev_calculate_function(train_df[label+'_Residual']) # calculate statistical value\n",
        "        test_min, test_max, test_mean, test_stdev =_mean_max_stdev_calculate_function(test_df[label+'_Residual']) # # calculate statistical value\n",
        "        \n",
        "        train_ab_min, train_ab_max, train_ab_mean, train_ab_stdev =_mean_max_stdev_calculate_function(train_df[label+'_Absolute_Residual']) # calculate statistical valu\n",
        "        test_ab_min, test_ab_max, test_ab_mean, test_ab_stdev =_mean_max_stdev_calculate_function(test_df[label+'_Absolute_Residual']) # # calculate statistical value\n",
        " \n",
        "\n",
        "\n",
        "        if len(model.evals_result().keys()) ==1: # model have only 1 key means only have  training  \"eval_set\" ( see function hyperparameter discription \"model\")\n",
        "\n",
        "            score_type = list(model.evals_result()['validation_0'].keys())[0] # find the \"eval_metric\" type used in this model\n",
        "            model_train_score = model.evals_result()['validation_0'][score_type][-1] # y-axis evaluation training result scores\n",
        "\n",
        "            ## Plot 2. Distplot with Distibution Plot [Absolute_Residual = abs(\"Predict\" - \"Actual\")] ( Turn on to draw ) ##\n",
        "            fig,ax1 = plt.subplots(figsize=(15,15))\n",
        "            sns.distplot(train_df[label+'_Absolute_Residual'], ax=ax1, color='b', kde=False)\n",
        "            sns.distplot(test_df[label+'_Absolute_Residual'], ax=ax1, color='r', kde=False)\n",
        "            ax1.axvline(3*(train_ab_stdev + test_ab_stdev)/2, \n",
        "                        linestyle='--',\n",
        "                        color='r')  # Plot +ve 3* sigma \n",
        "            plt.xlabel('Residual Value')\n",
        "            plt.ylabel('Counts')\n",
        "            plt.legend(['+3*sigma',\n",
        "                        '<Train Residue> mean:'+str(train_ab_mean)+' std:'+str(train_ab_stdev)+' min:'+str(train_ab_min)+' max:'+str(train_ab_max)+' '+str(score_type)+':'+str(model_train_score),\n",
        "                        '<Test Residue> mean:'+str(test_ab_mean)+' std:'+str(test_ab_stdev)+' min:'+str(test_ab_min)+' max:'+str(test_ab_max)\n",
        "                       ])\n",
        "            plt.title('Model Residual Error Distribution Plot [Absolute_Residue = abs(\"Actual\" - \"Predicted\")]')\n",
        "\n",
        "            \n",
        "            ## Plot 3. Distplot with Density Plot ( Turn on to draw ) ##\n",
        "            fig,ax1 = plt.subplots(figsize=(15,15))\n",
        "            sns.distplot(train_df[label+'_Absolute_Residual'], ax=ax1, color='b', kde=True)\n",
        "            sns.distplot(test_df[label+'_Absolute_Residual'], ax=ax1, color='r', kde=True)\n",
        "            ax1.axvline(3*(train_ab_stdev + test_ab_stdev)/2, \n",
        "                        linestyle='--',\n",
        "                        color='r')  # Plot +ve 3* sigma \n",
        "            plt.xlabel('Residual Value')\n",
        "            plt.ylabel('Density')\n",
        "            plt.legend(['Train Cummulative Density Line',\n",
        "                        'Test Cummulative Density Line',\n",
        "                        '+3*sigma',\n",
        "                        '<Train Residue> mean:'+str(train_ab_mean)+' std:'+str(train_ab_stdev)+' min:'+str(train_ab_min)+' max:'+str(train_ab_max)+' '+str(score_type)+':'+str(model_train_score),\n",
        "                        '<Test Residue> mean:'+str(test_ab_mean)+' std:'+str(test_ab_stdev)+' min:'+str(test_ab_min)+' max:'+str(test_ab_max)\n",
        "                       ])\n",
        "            plt.title('Model Residual Error Density Plot [Absolute_Residue = abs(\"Actual\" - \"Predicted\")]')\n",
        "\n",
        "            ## Histplot  ( Turn on to draw ) ##\n",
        "    #         fig,ax1 = plt.subplots(figsize=(15,15))\n",
        "    #         ax1.hist(train_df[label+'_Residual'], color='b')\n",
        "    #         ax1.hist(test_df[label+'_Residual'], color='r')\n",
        "    #         plt.legend(['Train','Test'])\n",
        "    #         plt.title('Model Residual Error Distribution Plot [Residue = abs(\"Actual\" - \"Predicted\")]')\n",
        "        \n",
        "        \n",
        "\n",
        "            ## Plot 4. Distplot with Distibution Plot [Residual = \"Predict\" - \"Actual\"] ( Turn on to draw ) ##\n",
        "            fig,ax3 = plt.subplots(figsize=(15,15))\n",
        "            sns.distplot(train_df[label+'_Residual'], ax=ax3, color='b', kde=False)\n",
        "            sns.distplot(test_df[label+'_Residual'], ax=ax3, color='r', kde=False)\n",
        "            ax3.axvline(3*(train_stdev + test_stdev)/2, \n",
        "                        linestyle='--',\n",
        "                        color='r')  # Plot +ve 3* sigma \n",
        "            ax3.axvline(-3*(train_stdev + test_stdev)/2, \n",
        "                        linestyle='--',\n",
        "                        color='m')  # Plot +ve 3* sigma \n",
        "            plt.xlabel('Residual Value')\n",
        "            plt.ylabel('Counts')\n",
        "            plt.legend(['+3*sigma',\n",
        "                        '-3*sigma',\n",
        "                        '<Train Residue> mean:'+str(train_mean)+' std:'+str(train_stdev)+' min:'+str(train_min)+' max:'+str(train_max)+' '+str(score_type)+':'+str(model_train_score),\n",
        "                        '<Test Residue> mean:'+str(test_mean)+' std:'+str(test_stdev)+' min:'+str(test_min)+' max:'+str(test_max)\n",
        "                       ])\n",
        "            plt.title('Model Residual Error Distribution Plot [Residue = \"Actual\" - \"Predicted\"]')\n",
        "            \n",
        "       \n",
        "        else: # else model have only 2 key means only have  training and testing \"eval_set\" ( see function hyperparameter discription \"model\")\n",
        "            \n",
        "            score_type = list(model.evals_result()['validation_0'].keys())[0] # find the \"eval_metric\" type used in this model\n",
        "            model_train_score = model.evals_result()['validation_0'][score_type][-1] # y-axis evaluation Training result scores\n",
        "            model_test_score = model.evals_result()['validation_1'][score_type][-1] # y-axis evaluation Testing result scores\n",
        "            \n",
        "            \n",
        "            ## Plot 2. Distplot with Distibution Plot [Absolute_Residual = abs(\"Predict\" - \"Actual\")] ( Turn on to draw ) ##\n",
        "            fig,ax1 = plt.subplots(figsize=(15,15))\n",
        "            sns.distplot(train_df[label+'_Absolute_Residual'], ax=ax1, color='b', kde=False)\n",
        "            sns.distplot(test_df[label+'_Absolute_Residual'], ax=ax1, color='r', kde=False)\n",
        "            ax1.axvline(3*(train_ab_stdev + test_ab_stdev)/2, \n",
        "                        linestyle='--',\n",
        "                        color='r')  # Plot +ve 3* sigma \n",
        "            plt.xlabel('Residual Value')\n",
        "            plt.ylabel('Counts')\n",
        "            plt.legend(['+3*sigma',\n",
        "                        '<Train Residue> mean:'+str(train_ab_mean)+' std:'+str(train_ab_stdev)+' min:'+str(train_ab_min)+' max:'+str(train_ab_max)+' '+str(score_type)+':'+str(model_train_score),\n",
        "                        '<Test Residue> mean:'+str(test_ab_mean)+' std:'+str(test_ab_stdev)+' min:'+str(test_ab_min)+' max:'+str(test_ab_max)+' '+str(score_type)+':'+str(model_test_score)\n",
        "                       ])\n",
        "            plt.title('Model Residual Error Distribution Plot [Absolute_Residue = abs(\"Actual\" - \"Predicted\")]')\n",
        "\n",
        "            \n",
        "            ## Plot 3. Distplot with Density Plot ( Turn on to draw ) ##\n",
        "            fig,ax1 = plt.subplots(figsize=(15,15))\n",
        "            sns.distplot(train_df[label+'_Absolute_Residual'], ax=ax1, color='b', kde=True)\n",
        "            sns.distplot(test_df[label+'_Absolute_Residual'], ax=ax1, color='r', kde=True)\n",
        "            ax1.axvline(3*(train_ab_stdev + test_ab_stdev)/2, \n",
        "                        linestyle='--',\n",
        "                        color='r')  # Plot +ve 3* sigma \n",
        "            plt.xlabel('Residual Value')\n",
        "            plt.ylabel('Density')\n",
        "            plt.legend(['Train Cummulative Density Line',\n",
        "                        'Test Cummulative Density Line',\n",
        "                        '+3*sigma',\n",
        "                        '<Train Residue> mean:'+str(train_ab_mean)+' std:'+str(train_ab_stdev)+' min:'+str(train_ab_min)+' max:'+str(train_ab_max)+' '+str(score_type)+':'+str(model_train_score),\n",
        "                        '<Test Residue> mean:'+str(test_ab_mean)+' std:'+str(test_ab_stdev)+' min:'+str(test_ab_min)+' max:'+str(test_ab_max)+' '+str(score_type)+':'+str(model_test_score)\n",
        "                       ])\n",
        "            plt.title('Model Residual Error Density Plot [Absolute_Residue = abs(\"Actual\" - \"Predicted\")]')\n",
        "\n",
        "            ## Histplot  ( Turn on to draw ) ##\n",
        "    #         fig,ax1 = plt.subplots(figsize=(15,15))\n",
        "    #         ax1.hist(train_df[label+'_Residual'], color='b')\n",
        "    #         ax1.hist(test_df[label+'_Residual'], color='r')\n",
        "    #         plt.legend(['Train','Test'])\n",
        "    #         plt.title('Model Residual Error Distribution Plot [Residue = abs(\"Actual\" - \"Predicted\")]')\n",
        "        \n",
        "            \n",
        "            ## Plot 4. Distplot with Distibution Plot [Residual = \"Predict\" - \"Actual\"] ( Turn on to draw ) ##\n",
        "            fig,ax3 = plt.subplots(figsize=(15,15))\n",
        "            sns.distplot(train_df[label+'_Residual'], ax=ax3, color='b', kde=False)\n",
        "            sns.distplot(test_df[label+'_Residual'], ax=ax3, color='r', kde=False)\n",
        "            ax3.axvline(3*(train_stdev + test_stdev)/2, \n",
        "                        linestyle='--',\n",
        "                        color='r')  # Plot +ve 3* sigma \n",
        "            ax3.axvline(-3*(train_stdev + test_stdev)/2, \n",
        "                        linestyle='--',\n",
        "                        color='m')  # Plot +ve 3* sigma \n",
        "            plt.xlabel('Residual Value')\n",
        "            plt.ylabel('Counts')\n",
        "            plt.legend(['+3*sigma',\n",
        "                        '-3*sigma',\n",
        "                        '<Train Residue> mean:'+str(train_mean)+' std:'+str(train_stdev)+' min:'+str(train_min)+' max:'+str(train_max)+' '+str(score_type)+':'+str(model_train_score),\n",
        "                        '<Test Residue> mean:'+str(test_mean)+' std:'+str(test_stdev)+' min:'+str(test_min)+' max:'+str(test_max)+' '+str(score_type)+':'+str(model_test_score)\n",
        "                       ])\n",
        "            plt.title('Model Residual Error Distribution Plot [Residue = \"Actual\" - \"Predicted\"]')\n",
        "\n",
        "        \n",
        "        return train_df,test_df\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def _train_test_evals_result_plot(model):\n",
        "        \n",
        "        ### Plot Model Train Test Lost Function Trends ### \n",
        "        \n",
        "        \n",
        "        if len(model.evals_result().keys()) ==1:\n",
        "\n",
        "            score_type = list(model.evals_result()['validation_0'].keys())[0] # find the \"eval_metric\" type used in this model\n",
        "            y_axis_1 = model.evals_result()['validation_0'][score_type] # y-axis evaluation training result scores\n",
        "            epochs = len(y_axis_1) # x_axis number of epochs\n",
        "            x_axis= range(0,epochs)\n",
        "\n",
        "            final_train_score =y_axis_1[-1] # final model training dataset epoch score\n",
        "\n",
        "            ## plot figure ##\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(15,15))\n",
        "            ax.plot(x_axis, y_axis_1, label='Training_'+score_type , marker='o', color='b' )\n",
        "            plt.text(x_axis[-1]+1, y_axis_1[-1], '[Last Epochs Train '+score_type+' : '+str(final_train_score)+']') # Add final rmse value test to plot graph\n",
        "            ax.legend()\n",
        "            plt.xlabel('Rounds [ Epochs ]')\n",
        "            plt.ylabel('Lost Function [ '+score_type+ ' ]')\n",
        "            plt.title('Model Train Test Lost Function Trends ')\n",
        "            plt.grid()\n",
        "            \n",
        "            return\n",
        "\n",
        "        elif len(model.evals_result().keys()) == 2:\n",
        "\n",
        "            score_type = list(model.evals_result()['validation_0'].keys())[0] # find the \"eval_metric\" type used in this model\n",
        "            y_axis_1 = model.evals_result()['validation_0'][score_type] # y-axis evaluation Training result scores\n",
        "            y_axis_2 = model.evals_result()['validation_1'][score_type] # y-axis evaluation Testing result scores\n",
        "            epochs = len(y_axis_1) # x_axis number of epochs\n",
        "            x_axis= range(0,epochs)\n",
        "            final_train_score =y_axis_1[-1] # final model training dataset epoch score\n",
        "            final_test_score =y_axis_2[-1] # final model testing dataset epoch score\n",
        "\n",
        "            ## plot figure ## \n",
        "            fig,ax = plt.subplots(figsize=(15,15))\n",
        "            ax.plot(x_axis, y_axis_1, label='Training_dataset_'+score_type,  marker = 'o', color='b')\n",
        "            plt.text(x_axis[-1]+1, y_axis_1[-1], '[Last Epochs Train '+score_type+' : '+str(final_train_score)+']')\n",
        "\n",
        "            ax.plot(x_axis, y_axis_2, label='Testing_dataset_'+score_type,  marker='o', color='r')\n",
        "            plt.text(x_axis[-1]+1, y_axis_2[-1], '[Last Epochs Test '+score_type+' : '+str(final_test_score)+']')\n",
        "\n",
        "            ax.legend()\n",
        "            plt.xlabel('Rounds [ Epochs ]')\n",
        "            plt.ylabel('Lost Function [ '+score_type+ ' ]')\n",
        "            plt.title('Model Train Test Lost Function Trends ')\n",
        "            plt.grid()\n",
        "            \n",
        "            return\n",
        "            \n",
        "        else:\n",
        "            print(\"Model Evals_result more 2 types [ Training and Testing ] ... edit code on  [ XGB_model_evaluation_function ] \")\n",
        "        \n",
        "            return\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    def _analyze_over_under_reject_function(train_df,test_df,label,output_spec_limits, over_under_reject_analyze_feature):\n",
        "        ### 1. Plot Over Reject Graph ###\n",
        "        ### 2. Plot Over Reject Graph ###\n",
        "            # Plot 1 -  [TRAINING Dataset] UNDER REJECT Scatter Plot\n",
        "            # Plot 2 -  [TESTING Dataset] UNDER REJECT Scatter Plot\n",
        "            # Plot 3 -  [TRAINING Dataset] OVER REJECT Scatter Plot\n",
        "            # Plot 4 -  [TESTING Dataset] OVER REJECT Scatter Plot\n",
        "        \n",
        "        ### Conditonal ###\n",
        "        \n",
        "        ## under reject = [ predicted output < output_spec_limits ] & [ actual output > output_spec_limits ]\n",
        "        ## Over reject = [ predicted output > output_spec_limits ] & [ actual output < output_spec_limits ] \n",
        "        ## rejection threshold = Ground Truth value \n",
        "        ## distinguish_feature = distinguish over reject by features class\n",
        "        \n",
        "        ### Hyper Parameter ###\n",
        "        \n",
        "        ## 1. train_df == train dataframe\n",
        "        ## 2. test_df == test dataframe\n",
        "        ## 3. label == output label features name\n",
        "        ## 4. output_spec_limits = output spec limit\n",
        "                # is a list = [lower spec limit , upper spec limit ]\n",
        "                # if no spec = 'Null' , Ex : [ 35, 'Null']\n",
        "        ## 5. over_under_reject_analyze_feature == analyze feature name, it is a single index list ['label_name'] \n",
        "            # if 'Null' = does not plot according to features class\n",
        "            #'feature name' = plot according to feature name class\n",
        "        \n",
        "        \n",
        "\n",
        "        train_under_df = train_df.copy()\n",
        "        train_over_df = train_df.copy()\n",
        "        \n",
        "        test_under_df = test_df.copy()\n",
        "        test_over_df = test_df.copy()\n",
        "        \n",
        "        \n",
        "        ## Train Dataset Under/Over Reject Analysis ##\n",
        "        if over_under_reject_analyze_feature[0] =='Null': # When over_under_reject_analyze_feature is NOT GIVEN == \" NULL\"\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' and output_spec_limits[1] != 'Null': # output_spec_limits have \" upper spec\" + \" Lower Spec\"\n",
        "\n",
        "                ## Under Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output > output_spec_limits ]\n",
        "                train_under_df = train_under_df.loc[ (train_df[label+'_Actual'] < output_spec_limits[0] ) | (train_df[label+'_Actual'] > output_spec_limits[1]),\n",
        "                                              [label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Train dataset [ predicted output < output_spec_limits ]\n",
        "                train_under_df = train_under_df.loc[(train_under_df[label+'_Predict'] >= output_spec_limits[0]) & (train_under_df[label+'_Predict'] <= output_spec_limits[1]),\n",
        "                                                    [label+'_Actual',label+'_Predict'] ]\n",
        "                \n",
        "                # Test Dataset [ actual output > output_spec_limits ]\n",
        "                test_under_df = test_under_df.loc[ (test_df[label+'_Actual'] < output_spec_limits[0] ) | (test_df[label+'_Actual'] > output_spec_limits[1]),\n",
        "                                            [label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Test Dataset [ predicted output < output_spec_limits ]\n",
        "                test_under_df = test_under_df.loc[(test_under_df[label+'_Predict'] >= output_spec_limits[0]) & (test_under_df[label+'_Predict'] <= output_spec_limits[1]),\n",
        "                                                  [label+'_Actual',label+'_Predict'] ]\n",
        "                \n",
        "                \n",
        "                \n",
        "                ## Over Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output < output_spec_limits ] \n",
        "                train_over_df = train_over_df.loc[ (train_df[label+'_Actual'] >= output_spec_limits[0] ) & (train_df[label+'_Actual'] <= output_spec_limits[1]),\n",
        "                                             [label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Train dataset [ predicted output > output_spec_limits ]\n",
        "                train_over_df = train_over_df.loc[(train_over_df[label+'_Predict'] < output_spec_limits[0]) | (train_over_df[label+'_Predict'] > output_spec_limits[1]),\n",
        "                                                  [label+'_Actual',label+'_Predict'] ]\n",
        "                \n",
        "                # Test Dataset [ actual output < output_spec_limits ] \n",
        "                test_over_df = test_over_df.loc[ (test_df[label+'_Actual'] >= output_spec_limits[0] ) & (test_df[label+'_Actual'] <= output_spec_limits[1]),\n",
        "                                           [label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Test Dataset [ predicted output > output_spec_limits ]\n",
        "                test_over_df = test_over_df.loc[(test_over_df[label+'_Predict'] < output_spec_limits[0]) | (test_over_df[label+'_Predict'] > output_spec_limits[1]),\n",
        "                                                [label+'_Actual',label+'_Predict'] ]\n",
        "                \n",
        "                \n",
        "            elif output_spec_limits[0] != 'Null' and output_spec_limits[1] == 'Null': # output_spec_limits ONLY HAVE \" Lower Spec\" !!!!\n",
        "                \n",
        "                ## Under Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output > output_spec_limits ]\n",
        "                train_under_df = train_under_df.loc[ (train_df[label+'_Actual'] < output_spec_limits[0] ) & (train_under_df[label+'_Predict'] >= output_spec_limits[0]),\n",
        "                                              [label+'_Actual', label+'_Predict'] ]\n",
        "                \n",
        "                # Test Dataset [ actual output > output_spec_limits ]\n",
        "                test_under_df = test_under_df.loc[ (test_df[label+'_Actual'] < output_spec_limits[0] ) & (test_under_df[label+'_Predict'] >= output_spec_limits[0]),\n",
        "                                            [ label+'_Actual', label+'_Predict'] ] \n",
        "            \n",
        "                \n",
        "                ## Over Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output < output_spec_limits ] \n",
        "                train_over_df = train_over_df.loc[ (train_df[label+'_Actual'] >= output_spec_limits[0] ) & (train_over_df[label+'_Predict'] < output_spec_limits[0]),\n",
        "                                             [label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Test Dataset [ actual output < output_spec_limits ] \n",
        "                test_over_df = test_over_df.loc[ (test_df[label+'_Actual'] >= output_spec_limits[0] ) & (test_over_df[label+'_Predict'] < output_spec_limits[0]),\n",
        "                                           [label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                \n",
        "                \n",
        "            elif output_spec_limits[0] == 'Null' and output_spec_limits[1] != 'Null': # output_spec_limits ONLY HAVE \" Upper Spec\" !!!!\n",
        "                \n",
        "                ## Under Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output > output_spec_limits ]\n",
        "                train_under_df = train_under_df.loc[ (train_df[label+'_Actual'] > output_spec_limits[1] ) & (train_under_df[label+'_Predict'] <= output_spec_limits[1]),\n",
        "                                              [label+'_Actual', label+'_Predict'] ]\n",
        "                \n",
        "                # Test Dataset [ actual output > output_spec_limits ]\n",
        "                test_under_df = test_under_df.loc[ (test_df[label+'_Actual'] > output_spec_limits[1] ) & (test_under_df[label+'_Predict'] <= output_spec_limits[1]),\n",
        "                                            [label+'_Actual', label+'_Predict'] ] \n",
        "            \n",
        "                \n",
        "                ## Over Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output < output_spec_limits ] \n",
        "                train_over_df = train_over_df.loc[ (train_df[label+'_Actual'] <= output_spec_limits[1] ) & (train_over_df[label+'_Predict'] > output_spec_limits[1]),\n",
        "                                             [label+'_Actual', label+'_Predict'] ] \n",
        "             \n",
        "                # Test Dataset [ actual output < output_spec_limits ] \n",
        "                test_over_df = test_over_df.loc[ (test_df[label+'_Actual'] <= output_spec_limits[1] ) & (test_over_df[label+'_Predict'] > output_spec_limits[1]),\n",
        "                                           [label+'_Actual', label+'_Predict'] ]\n",
        "                \n",
        "            else: # Empty output_spec_limits !!!!\n",
        "                Print(\"WARNING : output_spec_limits is Empty !!! , Please input Spec limit Threshold before calling the function\")\n",
        "            \n",
        "            # add plot index column to all dataframe\n",
        "            train_under_df['plot_index'] = range(1,len(train_under_df)+1)\n",
        "            test_under_df['plot_index'] = range(1,len(test_under_df)+1)\n",
        "            train_over_df['plot_index'] = range(1,len(train_over_df)+1)\n",
        "            test_over_df['plot_index'] = range(1,len(test_over_df)+1)\n",
        "            \n",
        "            \n",
        "            legend_name = [\"Predicted\", \"Actual\"] # legend naming list \n",
        "            \n",
        "            ## Plot 1 -  [TRAINING Dataset] UNDER REJECT Scatter Plot ##\n",
        "            fig, ax1 = plt.subplots(figsize =(25,18))\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' : # plot lower spec limit line\n",
        "                ax1.axhline(output_spec_limits[0], \n",
        "                           linestyle='--',\n",
        "                           color='r') # Predicted Ground Truth Value\n",
        "                legend_name.insert(0,'Lower Specs Limit') # if there is lower spec limit add lower spec limit legend name\n",
        "                \n",
        "                \n",
        "            if output_spec_limits[1] != 'Null' : # plot upper spec limit line \n",
        "\n",
        "                ax1.axhline(output_spec_limits[1], \n",
        "                           linestyle='--',\n",
        "                           color='m') # Predicted Ground Truth Value\n",
        "                legend_name.insert(1,'Upper Specs Limit') # if there is upper spec limit add lower spec limit legend name\n",
        "                \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Predict', data=train_under_df , ax = ax1, s=60, marker='o') \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Actual', data=train_under_df , ax = ax1 , s=60, marker='^' ) \n",
        "            plt.legend(legend_name, loc='upper right')\n",
        "            plt.title('Model Prediction UNDER REJECT (TRAINING Dataset)')\n",
        "            plt.xlabel('No')\n",
        "            plt.ylabel(label)\n",
        "            \n",
        "            \n",
        "            ## Plot 2 -  [TESTING Dataset] UNDER REJECT Scatter Plot ##\n",
        "            fig, ax2 = plt.subplots(figsize =(25,18))\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' : # plot lower spec limit line\n",
        "                ax2.axhline(output_spec_limits[0], \n",
        "                           linestyle='--',\n",
        "                           color='r') # Predicted Ground Truth Value\n",
        "                \n",
        "            if output_spec_limits[1] != 'Null' : # plot upper spec limit line \n",
        "\n",
        "                ax2.axhline(output_spec_limits[1], \n",
        "                           linestyle='--',\n",
        "                           color='m') # Predicted Ground Truth Value\n",
        "            \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Predict', data=test_under_df , ax = ax2, s=60, marker='o')# scatter plot - predicted value\n",
        "            sns.scatterplot(x='plot_index', y=label+'_Actual', data=test_under_df , ax = ax2 , s=60, marker='^') # scatter plot - actual value\n",
        "            plt.legend(legend_name,loc='upper right')\n",
        "            plt.title('Model Prediction UNDER REJECT (TESTING Dataset)')\n",
        "            plt.xlabel('No')\n",
        "            plt.ylabel(label)\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            ## Plot 3 -  [TRAINING Dataset] OVER REJECT Scatter Plot ##\n",
        "            fig, ax3 = plt.subplots(figsize =(25,18))\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' : # plot lower spec limit line\n",
        "                ax3.axhline(output_spec_limits[0], \n",
        "                           linestyle='--',\n",
        "                           color='r') # Predicted Ground Truth Value\n",
        "                \n",
        "            if output_spec_limits[1] != 'Null' : # plot upper spec limit line \n",
        "\n",
        "                ax3.axhline(output_spec_limits[1], \n",
        "                           linestyle='--',\n",
        "                           color='m') # Predicted Ground Truth Value\n",
        "                \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Predict', data=train_over_df , ax = ax3, s=60, marker='o') # scatter plot - predicted value\n",
        "            sns.scatterplot(x='plot_index', y=label+'_Actual', data=train_over_df , ax = ax3 , s=60, marker='^') # scatter plot - actual value\n",
        "            plt.legend(legend_name,loc='upper right')\n",
        "            plt.title('Model Prediction OVER REJECT (TRAINING Dataset)')\n",
        "            plt.xlabel('No')\n",
        "            plt.ylabel(label)\n",
        "            \n",
        "            \n",
        "            \n",
        "            ## Plot 4 -  [TESTING Dataset] OVER REJECT Scatter Plot ##\n",
        "            fig, ax4 = plt.subplots(figsize =(25,18))\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' : # plot lower spec limit line\n",
        "                ax4.axhline(output_spec_limits[0], \n",
        "                           linestyle='--',\n",
        "                           color='r') # Predicted Ground Truth Value\n",
        "                \n",
        "            if output_spec_limits[1] != 'Null' : # plot upper spec limit line \n",
        "\n",
        "                ax4.axhline(output_spec_limits[1], \n",
        "                           linestyle='--',\n",
        "                           color='m') # Predicted Ground Truth Value\n",
        "                \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Predict', data=test_over_df , ax = ax4, s=60, marker='o') # scatter plot - predicted value        \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Actual', data=test_over_df , ax = ax4 , s=60, marker='^') # scatter plot - actual value\n",
        "            plt.legend(legend_name,loc='upper right')\n",
        "            plt.title('Model Prediction OVER REJECT (TRAINING Dataset)')\n",
        "            plt.xlabel('No')\n",
        "            plt.ylabel(label)\n",
        "            \n",
        "\n",
        "            \n",
        "        else:  # When over_under_reject_analyze_feature is GIVEN != \" NULL\"\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' and output_spec_limits[1] != 'Null': # output_spec_limits have \" upper spec\" + \" Lower Spec\"\n",
        "\n",
        "                ## Under Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output > output_spec_limits ]\n",
        "                train_under_df = train_under_df.loc[ (train_df[label+'_Actual'] < output_spec_limits[0] ) | (train_df[label+'_Actual'] > output_spec_limits[1]),\n",
        "                                              [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Train dataset [ predicted output < output_spec_limits ]\n",
        "                train_under_df = train_under_df.loc[(train_under_df[label+'_Predict'] >= output_spec_limits[0]) & (train_under_df[label+'_Predict'] <= output_spec_limits[1]),\n",
        "                                                    [over_under_reject_analyze_feature[0],label+'_Actual',label+'_Predict'] ]\n",
        "                \n",
        "                # Test Dataset [ actual output > output_spec_limits ]\n",
        "                test_under_df = test_under_df.loc[ (test_df[label+'_Actual'] < output_spec_limits[0] ) | (test_df[label+'_Actual'] > output_spec_limits[1]),\n",
        "                                            [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Test Dataset [ predicted output < output_spec_limits ]\n",
        "                test_under_df = test_under_df.loc[(test_under_df[label+'_Predict'] >= output_spec_limits[0]) & (test_under_df[label+'_Predict'] <= output_spec_limits[1]),\n",
        "                                                  [over_under_reject_analyze_feature[0],label+'_Actual',label+'_Predict'] ]\n",
        "                \n",
        "                \n",
        "                \n",
        "                ## Over Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output < output_spec_limits ] \n",
        "                train_over_df = train_over_df.loc[ (train_df[label+'_Actual'] >= output_spec_limits[0] ) & (train_df[label+'_Actual'] <= output_spec_limits[1]),\n",
        "                                             [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Train dataset [ predicted output > output_spec_limits ]\n",
        "                train_over_df = train_over_df.loc[(train_over_df[label+'_Predict'] < output_spec_limits[0]) | (train_over_df[label+'_Predict'] > output_spec_limits[1]),\n",
        "                                                  [over_under_reject_analyze_feature[0],label+'_Actual',label+'_Predict'] ]\n",
        "                \n",
        "                # Test Dataset [ actual output < output_spec_limits ] \n",
        "                test_over_df = test_over_df.loc[ (test_df[label+'_Actual'] >= output_spec_limits[0] ) & (test_df[label+'_Actual'] <= output_spec_limits[1]),\n",
        "                                           [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Test Dataset [ predicted output > output_spec_limits ]\n",
        "                test_over_df = test_over_df.loc[(test_over_df[label+'_Predict'] < output_spec_limits[0]) | (test_over_df[label+'_Predict'] > output_spec_limits[1]),\n",
        "                                                [over_under_reject_analyze_feature[0],label+'_Actual',label+'_Predict'] ]\n",
        "                \n",
        "                \n",
        "            elif output_spec_limits[0] != 'Null' and output_spec_limits[1] == 'Null': # output_spec_limits ONLY HAVE \" Lower Spec\" !!!!\n",
        "                \n",
        "                ## Under Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output > output_spec_limits ]\n",
        "                train_under_df = train_under_df.loc[ (train_df[label+'_Actual'] < output_spec_limits[0] ) & (train_under_df[label+'_Predict'] >= output_spec_limits[0]),\n",
        "                                              [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ]\n",
        "                \n",
        "                # Test Dataset [ actual output > output_spec_limits ]\n",
        "                test_under_df = test_under_df.loc[ (test_df[label+'_Actual'] < output_spec_limits[0] ) & (test_under_df[label+'_Predict'] >= output_spec_limits[0]),\n",
        "                                            [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "            \n",
        "                \n",
        "                ## Over Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output < output_spec_limits ] \n",
        "                train_over_df = train_over_df.loc[ (train_df[label+'_Actual'] >= output_spec_limits[0] ) & (train_over_df[label+'_Predict'] < output_spec_limits[0]),\n",
        "                                             [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                # Test Dataset [ actual output < output_spec_limits ] \n",
        "                test_over_df = test_over_df.loc[ (test_df[label+'_Actual'] >= output_spec_limits[0] ) & (test_over_df[label+'_Predict'] < output_spec_limits[0]),\n",
        "                                           [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "                \n",
        "                \n",
        "                \n",
        "            elif output_spec_limits[0] == 'Null' and output_spec_limits[1] != 'Null': # output_spec_limits ONLY HAVE \" Upper Spec\" !!!!\n",
        "                \n",
        "                ## Under Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output > output_spec_limits ]\n",
        "                train_under_df = train_under_df.loc[ (train_df[label+'_Actual'] > output_spec_limits[1] ) & (train_under_df[label+'_Predict'] <= output_spec_limits[1]),\n",
        "                                              [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ]\n",
        "                \n",
        "                # Test Dataset [ actual output > output_spec_limits ]\n",
        "                test_under_df = test_under_df.loc[ (test_df[label+'_Actual'] > output_spec_limits[1] ) & (test_under_df[label+'_Predict'] <= output_spec_limits[1]),\n",
        "                                            [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "            \n",
        "                \n",
        "                ## Over Reject ##\n",
        "                \n",
        "                # Train dataset [ actual output < output_spec_limits ] \n",
        "                train_over_df = train_over_df.loc[ (train_df[label+'_Actual'] <= output_spec_limits[1] ) & (train_over_df[label+'_Predict'] > output_spec_limits[1]),\n",
        "                                             [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ] \n",
        "             \n",
        "                # Test Dataset [ actual output < output_spec_limits ] \n",
        "                test_over_df = test_over_df.loc[ (test_df[label+'_Actual'] <= output_spec_limits[1] ) & (test_over_df[label+'_Predict'] > output_spec_limits[1]),\n",
        "                                           [over_under_reject_analyze_feature[0], label+'_Actual', label+'_Predict'] ]\n",
        "                \n",
        "            else: # Empty output_spec_limits !!!!\n",
        "                Print(\"WARNING : output_spec_limits is Empty !!! , Please input Spec limit Threshold before calling the function\")\n",
        "               \n",
        "            \n",
        "            # add plot index column to all dataframe\n",
        "            train_under_df['plot_index'] = range(1,len(train_under_df)+1)\n",
        "            test_under_df['plot_index'] = range(1,len(test_under_df)+1)\n",
        "            train_over_df['plot_index'] = range(1,len(train_over_df)+1)\n",
        "            test_over_df['plot_index'] = range(1,len(test_over_df)+1)\n",
        "            \n",
        "            \n",
        "            ## Plot 1 -  [TRAINING Dataset] UNDER REJECT Scatter Plot ##\n",
        "            fig, ax1 = plt.subplots(figsize =(25,18))\n",
        "            \n",
        "            spec_legend_name=[]\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' : # plot lower spec limit line\n",
        "                ax1.axhline(output_spec_limits[0], \n",
        "                           linestyle='--',\n",
        "                           color='r') # Predicted Ground Truth Value\n",
        "                spec_legend_name.insert(0,'Lower Specs Limit') # if there is lower spec limit add lower spec limit legend name\n",
        "                \n",
        "                \n",
        "            if output_spec_limits[1] != 'Null' : # plot upper spec limit line \n",
        "\n",
        "                ax1.axhline(output_spec_limits[1], \n",
        "                           linestyle='--',\n",
        "                           color='m') # Predicted Ground Truth Value\n",
        "                spec_legend_name.insert(1,'Upper Specs Limit') # if there is upper spec limit add lower spec limit legend name\n",
        "                \n",
        "\n",
        "            sns.scatterplot(x='plot_index', y=label+'_Predict', data=train_under_df , ax = ax1, s=60, marker='o', hue=over_under_reject_analyze_feature[0])\n",
        "            leg=plt.legend(loc='upper right', title=over_under_reject_analyze_feature[0]+' - Model Predicted')\n",
        "            ax1.add_artist(leg) # every increase in legend plot need to add an artist\n",
        "            \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Actual', data=train_under_df , ax = ax1 , s=60, marker='^', hue=over_under_reject_analyze_feature[0] ) #,  color='k')\n",
        "            \n",
        "            # actual scatter plot legend  \n",
        "            plt_2_legend_name = list(train_under_df[over_under_reject_analyze_feature[0]].unique())\n",
        "            plt_2 = [ plt.plot([], [], marker='^')[0] for i in range(len(plt_2_legend_name))]\n",
        "            leg_1=plt.legend(handles=plt_2, labels=plt_2_legend_name ,loc='upper left', title=over_under_reject_analyze_feature[0]+' - Model Actual')\n",
        "            ax1.add_artist(leg_1) # every increase in legend plot need to add an artist\n",
        "            \n",
        "            # Spec limit scatter plot legend  \n",
        "            upper_spec_legend_style = mlines.Line2D([],[], color='m', linestyle='--', label='Upper Specs Limit')\n",
        "            lower_spec_legend_style = mlines.Line2D([],[], color='r', linestyle='--',label='Lower Specs Limit')\n",
        "            plt.legend(handles=[lower_spec_legend_style, upper_spec_legend_style] ,loc='upper center', title=over_under_reject_analyze_feature[0]+' - Spec Limit')\n",
        "            \n",
        "            plt.title('Model Prediction UNDER REJECT (TRAINING Dataset)')\n",
        "            plt.xlabel('No')\n",
        "            plt.ylabel(label)\n",
        "            \n",
        "            \n",
        "            ## Plot 2 -  [TESTING Dataset] UNDER REJECT Scatter Plot ##\n",
        "            fig, ax2 = plt.subplots(figsize =(25,18))\n",
        "            spec_legend_name=[]\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' : # plot lower spec limit line\n",
        "                ax2.axhline(output_spec_limits[0], \n",
        "                           linestyle='--',\n",
        "                           color='r') # Predicted Ground Truth Value\n",
        "                spec_legend_name.insert(0,'Lower Specs Limit') # if there is lower spec limit add lower spec limit legend name\n",
        "                \n",
        "                \n",
        "            if output_spec_limits[1] != 'Null' : # plot upper spec limit line \n",
        "\n",
        "                ax2.axhline(output_spec_limits[1], \n",
        "                           linestyle='--',\n",
        "                           color='m') # Predicted Ground Truth Value\n",
        "                spec_legend_name.insert(1,'Upper Specs Limit') # if there is upper spec limit add lower spec limit legend name\n",
        "                \n",
        "        \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Predict', data=test_under_df , ax = ax2, s=60, marker='o', hue=over_under_reject_analyze_feature[0]) # scatter plot - predicted value\n",
        "            leg=plt.legend(loc='upper right', title=over_under_reject_analyze_feature[0]+' - Model Predicted')\n",
        "            ax2.add_artist(leg)\n",
        "            \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Actual', data=test_under_df , ax = ax2 , s=60, marker='^', hue=over_under_reject_analyze_feature[0] ) # scatter plot - actual value\n",
        "            \n",
        "            # how to plot multiple scatter plot and multiple legend in one plot link : \n",
        "            \n",
        "            # actual scatter plot legend  \n",
        "            plt_2_legend_name = list(test_under_df[over_under_reject_analyze_feature[0]].unique())\n",
        "            plt_2 = [ plt.plot([], [], marker='^')[0] for i in range(len(plt_2_legend_name))]\n",
        "            leg_1=plt.legend(handles=plt_2, labels=plt_2_legend_name ,loc='upper left', title=over_under_reject_analyze_feature[0]+' - Model Actual')\n",
        "            ax2.add_artist(leg_1) \n",
        "            \n",
        "            # Spec limit scatter plot legend  \n",
        "            upper_spec_legend_style = mlines.Line2D([],[], color='m', linestyle='--', label='Upper Specs Limit')\n",
        "            lower_spec_legend_style = mlines.Line2D([],[], color='r', linestyle='--',label='Lower Specs Limit')\n",
        "            plt.legend(handles=[lower_spec_legend_style, upper_spec_legend_style] ,loc='upper center', title=over_under_reject_analyze_feature[0]+' - Spec Limit')\n",
        "            \n",
        "            plt.title('Model Prediction UNDER REJECT (TESTING Dataset)')\n",
        "            plt.xlabel('No')\n",
        "            plt.ylabel(label)\n",
        "            \n",
        "            \n",
        "            ## Plot 3 -  [TRAINING Dataset] OVER REJECT Scatter Plot ##\n",
        "            fig, ax3 = plt.subplots(figsize =(25,18))\n",
        "            spec_legend_name=[]\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' : # plot lower spec limit line\n",
        "                ax3.axhline(output_spec_limits[0], \n",
        "                           linestyle='--',\n",
        "                           color='r') # Predicted Ground Truth Value\n",
        "                spec_legend_name.insert(0,'Lower Specs Limit') # if there is lower spec limit add lower spec limit legend name\n",
        "                \n",
        "                \n",
        "            if output_spec_limits[1] != 'Null' : # plot upper spec limit line \n",
        "\n",
        "                ax3.axhline(output_spec_limits[1], \n",
        "                           linestyle='--',\n",
        "                           color='m') # Predicted Ground Truth Value\n",
        "                spec_legend_name.insert(1,'Upper Specs Limit') # if there is upper spec limit add lower spec limit legend name\n",
        "                \n",
        "                \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Predict', data=train_over_df , ax = ax3, s=60, marker='o', hue=over_under_reject_analyze_feature[0]) # scatter plot - predicted value\n",
        "            leg=plt.legend(loc='upper right', title=over_under_reject_analyze_feature[0]+' - Model Predicted')\n",
        "            ax3.add_artist(leg)\n",
        "            \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Actual', data=train_over_df , ax = ax3 , s=60, marker='^', hue=over_under_reject_analyze_feature[0] ) # scatter plot - actual value\n",
        "            \n",
        "            # actual scatter plot legend  \n",
        "            plt_2_legend_name = list(train_over_df[over_under_reject_analyze_feature[0]].unique())\n",
        "            plt_2 = [ plt.plot([], [], marker='^')[0] for i in range(len(plt_2_legend_name))]\n",
        "            leg_1=plt.legend(handles=plt_2, labels=plt_2_legend_name ,loc='upper left', title=over_under_reject_analyze_feature[0]+' - Model Actual')\n",
        "            ax3.add_artist(leg_1)\n",
        "            \n",
        "            # Spec limit scatter plot legend  \n",
        "            upper_spec_legend_style = mlines.Line2D([],[], color='m', linestyle='--', label='Upper Specs Limit')\n",
        "            lower_spec_legend_style = mlines.Line2D([],[], color='r', linestyle='--',label='Lower Specs Limit')\n",
        "            plt.legend(handles=[lower_spec_legend_style, upper_spec_legend_style] ,loc='upper center', title=over_under_reject_analyze_feature[0]+' - Spec Limit')\n",
        "            \n",
        "            plt.title('Model Prediction OVER REJECT (TRAINING Dataset)')\n",
        "            plt.xlabel('No')\n",
        "            plt.ylabel(label)\n",
        "            \n",
        "            \n",
        "            ## Plot 4 -  [TESTING Dataset] OVER REJECT Scatter Plot ##\n",
        "            fig, ax4 = plt.subplots(figsize =(25,18))\n",
        "            spec_legend_name=[]\n",
        "            \n",
        "            if output_spec_limits[0] != 'Null' : # plot lower spec limit line\n",
        "                ax4.axhline(output_spec_limits[0], \n",
        "                           linestyle='--',\n",
        "                           color='r') # Predicted Ground Truth Value\n",
        "                spec_legend_name.insert(0,'Lower Specs Limit') # if there is lower spec limit add lower spec limit legend name\n",
        "                \n",
        "                \n",
        "            if output_spec_limits[1] != 'Null' : # plot upper spec limit line \n",
        "\n",
        "                ax4.axhline(output_spec_limits[1], \n",
        "                           linestyle='--',\n",
        "                           color='m') # Predicted Ground Truth Value\n",
        "                spec_legend_name.insert(1,'Upper Specs Limit') # if there is upper spec limit add lower spec limit legend name\n",
        "            \n",
        "            \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Predict', data=test_over_df , ax = ax4, s=60, marker='o', hue=over_under_reject_analyze_feature[0]) # scatter plot - predicted value\n",
        "            leg=plt.legend(loc='upper right', title=over_under_reject_analyze_feature[0]+' - Model Predicted')\n",
        "            ax4.add_artist(leg)\n",
        "            \n",
        "            sns.scatterplot(x='plot_index', y=label+'_Actual', data=test_over_df , ax = ax4 , s=60, marker='^', hue=over_under_reject_analyze_feature[0] ) # scatter plot - actual value\n",
        "            \n",
        "            # actual scatter plot legend  \n",
        "            plt_2_legend_name = list(test_over_df[over_under_reject_analyze_feature[0]].unique())\n",
        "            plt_2 = [ plt.plot([], [], marker='^')[0] for i in range(len(plt_2_legend_name))]\n",
        "            leg_1=plt.legend(handles=plt_2, labels=plt_2_legend_name ,loc='upper left', title=over_under_reject_analyze_feature[0]+' - Model Actual')\n",
        "            ax4.add_artist(leg_1)\n",
        "            \n",
        "             # Spec limit scatter plot legend  \n",
        "            upper_spec_legend_style = mlines.Line2D([],[], color='m', linestyle='--', label='Upper Specs Limit')\n",
        "            lower_spec_legend_style = mlines.Line2D([],[], color='r', linestyle='--',label='Lower Specs Limit')\n",
        "            plt.legend(handles=[lower_spec_legend_style, upper_spec_legend_style] ,loc='upper center', title=over_under_reject_analyze_feature[0]+' - Spec Limit')\n",
        "            \n",
        "            plt.title('Model Prediction OVER REJECT (TRAINING Dataset)')\n",
        "            plt.xlabel('No')\n",
        "            plt.ylabel(label)\n",
        "\n",
        "        return\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    train_df, test_df = _evaluate_train_test_model_function(model,label,output_spec_limits,train_df,test_df,X_train,x_test,Y_train,y_test)\n",
        "    \n",
        "    _train_test_evals_result_plot( model )  \n",
        "    \n",
        "    _analyze_over_under_reject_function (train_df, test_df,label, output_spec_limits, over_under_reject_analyze_feature)\n",
        "    \n",
        "            \n",
        "    plt.show()\n",
        "    return\n",
        "        "
      ],
      "id": "ecf29306",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "463bbab9"
      },
      "source": [
        ""
      ],
      "id": "463bbab9",
      "execution_count": null,
      "outputs": []
    }
  ]
}